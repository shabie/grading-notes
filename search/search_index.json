{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Grading Notes","text":"<p>Grading Notes is a simple Python package that leverages Large Language Models (LLMs) as automated judges for evaluating AI-generated answers against human-written grading criteria. The repo is based on the awesome post by Databricks. The idea is to guide LLMs wtih simple grading notes rather than provide full ground truth answers thereby lowering the cost of creating ground truth data.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>LLM-powered Evaluation: Harness the intelligence of LLMs guided by humans to evaluate AI-generated answers.</li> <li>Flexible AI Providers: Support for multiple LLM providers, including Anthropic's Claude and OpenAI's GPT models.</li> <li>Bulk Processing: Evaluate multiple questions and answers efficiently using CSV files.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>You can install the Grading Notes package using pip:</p> <pre><code>pip install grading-notes\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from grading_notes import GradingNote, evaluate, get_judge\n\n# set ANTHROPIC_API_KEY and OPENAI_API_KEY env. variables\n\n# create a judge\njudge = get_judge(provider=\"anthropic\", model=\"claude-3-5-sonnet-20240620\")\n\n# create a grading note\ngrade_note = GradingNote(\n    question=\"What is the capital of France?\",\n    grading_note=\"Answer is 'Paris'. Accept case-insensitive variations.\"\n)\n\n# evaluate an answer\nresult = evaluate(judge=judge, grading_note=grade_note, answer=\"Paris\")\nprint(result) # True\n</code></pre>"},{"location":"#more-complex-examples","title":"More complex examples","text":"<p>Here is one complex example inspired from the Databricks post (answer generated by Gemini):</p> <pre><code>grade_note = GradingNote(\nquestion=\"How do I drop all tables in a Unity Catalog schema?\",\n\ngrading_note=\"The response should contain steps to get all table names then \\\ndrop each of them. Alternatively the response can suggest dropping the entire \\\nschema with risks explained. The response should not treat tables as views.\"\n)\n\nanswer = \"\"\"Unfortunately, Unity Catalog doesn't currently provide a direct \nway to drop all tables within a schema with a single command. You'll need to \nlist the tables in the schema and then drop them individually.\n\nHere's a breakdown of how you could approach this using Databricks SQL:\n\n1. List the Tables in the Schema using Python notebook\n\nYou can use the SHOW TABLES command to get a list of tables within your schema:\n\n%sql\nSHOW TABLES IN your_db_name.your_schema_name;\n\n2. Drop Each Table\nYou can use the DROP TABLE command to drop each table:\n\ntables = spark.sql(\"SHOW TABLES IN your_db_name.your_schema_name\").collect()\n\nfor table in tables:\n    table_name = table[\"tableName\"]\n    spark.sql(f\"DROP TABLE your_db_name.your_schema_name.{table_name}\")\n\"\"\"\n\nresult = evaluate(judge=judge, grading_note=grade_note, answer=answer)\nprint(result) # True\n</code></pre>"},{"location":"#main-components","title":"Main Components","text":"<ul> <li><code>evaluate</code>: Function to evaluate an answer against a grading note. Optionally takes an answer.</li> <li><code>evaluate_from_csv</code>: Function to evaluate multiple questions and answers from a CSV file. Optionally takes an answer.</li> <li><code>GradingNote</code>: Represents the grading criteria for a specific question.</li> <li><code>Judge</code>: Represents the judge client for different AI providers.</li> <li><code>Evaluation</code>: Represents the evaluation result (Good or Bad).</li> <li><code>get_judge</code>: Function to create an Judge for different AI providers.</li> </ul>"},{"location":"#csv-evaluation","title":"CSV Evaluation","text":"<p>You can evaluate multiple questions and answers using a CSV file:</p> <pre><code>from grading_notes import get_judge, evaluate_from_csv\njudge = get_judge(provider=\"openai\", model=\"gpt-4-turbo-preview\")\nresults = evaluate_from_csv(judge=judge, csv_file=\"path/to/your/csv_file.csv\")\n</code></pre> <p>The CSV file should have columns <code>question</code>, <code>grading_note</code>, and <code>answer</code>.</p>"},{"location":"#langchain-integration","title":"Langchain integration","text":"<p>You can integrate Grading Notes with langchain's custom string evaluator (example). Here's an example of how to create a custom evaluator in Langchain using Grading Notes:</p> <pre><code>from typing import Any, Optional\nfrom langchain.evaluation import StringEvaluator\nfrom grading_notes import GradingNote, evaluate, get_judge\n\n\nclass GradingNotesEvaluator(StringEvaluator):\n    \"\"\"Evaluate predictions using Grading Notes.\"\"\"\n    def __init__(\n            self,\n            provider: str = \"anthropic\",\n            model: str = \"claude-3-5-sonnet-20240620\",\n        ):\n        self.judge = get_judge(provider=provider, model=model)\n\n    def _evaluate_strings(\n            self,\n            *,\n            prediction: str,\n            reference: Optional[str] = None,\n            input: Optional[str] = None,\n            **kwargs: Any,\n        ) -&gt; dict:\n\n        if not input or not reference or not prediction:\n            msg = \"'input' (question), 'reference' (grading note) and \\\n                'prediction' (answer being evaluated) are *all* required.\"\n            raise ValueError(msg)\n\n        grading_note = GradingNote(question=input, grading_note=reference)\n        result = evaluate(\n            judge=self.judge,\n            grading_note=grading_note,\n            answer=prediction,\n            )\n        return {\"score\": result}\n\n# Usage\nevaluator = GradingNotesEvaluator()\nresult = evaluator.evaluate_strings(\n    prediction=\"Paris\",\n    reference=\"Answer is 'Paris'. Accept case-insensitive variations.\",\n    input=\"What is the capital of France?\"\n)\nprint(result)  # {'score': True}\n</code></pre>"},{"location":"#customization","title":"Customization","text":"<p>The repo currently supports Anthropic and OpenAI through the instructor library.</p>"},{"location":"#environment-variables","title":"Environment Variables","text":"<p>Make sure to set the following environment variables:</p> <ul> <li><code>ANTHROPIC_API_KEY</code>: Your Anthropic API key</li> <li><code>OPENAI_API_KEY</code>: Your OpenAI API key</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the Apache 2.0 License.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions! Please see our Contributing Guide for more details.</p>"},{"location":"CONTRIBUTING/","title":"Contributing to Grading Notes","text":"<p>We welcome contributions to the Grading Notes project! Here are some guidelines to help you get started. This repository uses the hatchling python project manager.</p>"},{"location":"CONTRIBUTING/#reporting-issues","title":"Reporting Issues","text":"<p>If you find a bug or have a suggestion for improvement, please open an issue on our GitHub repository.</p>"},{"location":"CONTRIBUTING/#submitting-pull-requests","title":"Submitting Pull Requests","text":"<ol> <li>Fork the repository and create your branch from <code>main</code>.</li> <li>If you've added code that should be tested, add tests.</li> <li>Ensure the test suite passes.</li> <li>Make sure your code lints.</li> <li>Issue that pull request!</li> </ol>"},{"location":"CONTRIBUTING/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed</p>"}]}